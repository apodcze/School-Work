{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756e461a-8a12-46d8-90d0-3fc773f2a40b",
   "metadata": {},
   "source": [
    "# ResNet-18: CIFAR-10 & PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cf985-e399-4feb-bc92-7df8d42e8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify GPU to be used-\n",
    "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b440c-8504-4e2f-80b3-e49029b85def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e94d94-1fa2-49f9-9683-4a5eef63690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e59fb4-c467-4764-8ff0-7118f738b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are multiple devices (i.e., GPU cards)-\n",
    "print(f\"Number of GPU(s) available = {torch.cuda.device_count()}\")\n",
    "\n",
    "# Which GPU Is The Current GPU?\n",
    "# print(f\"current GPU: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Get the name of the current GPU-\n",
    "# print(f\"Current GPU name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "\n",
    "# Is PyTorch using a GPU?\n",
    "# print(f\"Is PyTorch using a GPU? {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"Current GPU name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"PyTorch does not have access to GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7ba68-ac04-4113-ba4c-a1593a4a5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration-\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Available device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16cf6317-59bd-467b-a2ff-1cbed96a7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyper-parameters\n",
    "batch_size = 256\n",
    "num_classes = 10\n",
    "num_epochs = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221db477-251f-41cc-9343-915d82e9a7ed",
   "metadata": {},
   "source": [
    "### CIFAR-10 data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43cf104-519f-4206-9f63-5753bfea893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training and test sets-\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "      transforms.RandomCrop(32, padding = 4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(\n",
    "        mean = (0.4914, 0.4822, 0.4465),\n",
    "        std = (0.0305, 0.0296, 0.0342)),\n",
    "     ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(\n",
    "        mean = (0.4942, 0.4846, 0.4498),\n",
    "        std = (0.0304, 0.0295, 0.0342)),\n",
    "     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fc5a6-d264-4ece-8272-b4d111a95883",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root = './data', train = True,\n",
    "    download = True, transform = transform_train\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root = './data', train = False,\n",
    "    download = True, transform = transform_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2c641b-fcf1-4c6c-9891-ed84f0f6682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset, batch_size = 256,\n",
    "    shuffle = True\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = test_dataset, batch_size = 256,\n",
    "    shuffle = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b5192-b185-4601-ae4a-433ce9edb667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len(train_dataset) = {len(train_dataset)} & len(test_dataset) = {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbdf9cf-86f8-441c-8325-f7dad7b89cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len(train_loader) = {len(train_loader)} & len(test_loader) = {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5329d-c1f4-47d0-8348-73fcf636d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset) / 256, len(test_dataset) / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3758b33-2d1a-477c-8194-cb5f42208b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_stddev(data_loader):\n",
    "    '''\n",
    "    Compute mean and standard-deviation across all channels for the input\n",
    "    data loader.\n",
    "    '''\n",
    "    # VAR(X) = E(X^2) - E(X) ^ 2\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    \n",
    "    for data, _ in data_loader:\n",
    "        channels_sum += torch.mean(data, dim = [0, 2, 3])\n",
    "        # We don't want mean across channels (1st dimension), hence it is ignored.\n",
    "        \n",
    "        channels_squared_sum += torch.mean(data ** 2, dim = [0, 2, 3])\n",
    "        num_batches += 1\n",
    "    \n",
    "    mean = channels_sum / num_batches\n",
    "    std_dev = (channels_squared_sum / num_batches - (mean ** 2)) * 0.5\n",
    "    # You cannot sum the standard deviation as it is not a linear operation.\n",
    "    \n",
    "    return mean, std_dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29d736-5a05-4bfd-8630-561d2243c4c4",
   "metadata": {},
   "source": [
    "### Define _ResNet-18_ CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71f9ef7c-4e87-49d1-8189-83c253c0ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_Block(nn.Module):\n",
    "    '''\n",
    "    VGG block within a VGG-* CNN model\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, num_inp_channels,\n",
    "        num_channels, stride = 1,\n",
    "        dropout = 0.2, use_1x1_conv = False\n",
    "    ):\n",
    "        super(ResNet_Block, self).__init__()\n",
    "        \n",
    "        self.num_inp_channels = num_inp_channels\n",
    "        self.num_channels = num_channels\n",
    "        self.stride = stride\n",
    "        self.dropout = dropout\n",
    "        self.use_1x1_conv = use_1x1_conv\n",
    "    \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = self.num_inp_channels, out_channels = self.num_channels,\n",
    "            kernel_size = 3, padding = 1,\n",
    "            stride = self.stride, bias = False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = self.num_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = self.num_channels, out_channels = self.num_channels,\n",
    "            kernel_size = 3, padding = 1,\n",
    "            stride = 1, bias = False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(num_features = self.num_channels)\n",
    "        self.dropout = nn.Dropout(p = self.dropout)\n",
    "        \n",
    "        if self.use_1x1_conv:\n",
    "            self.conv3 = nn.Conv2d(\n",
    "            in_channels = self.num_inp_channels, out_channels = num_channels,\n",
    "            kernel_size = 1, padding = 0,\n",
    "            stride = self.stride, bias = False\n",
    "            )\n",
    "            self.bn3 = nn.BatchNorm2d(num_features = self.num_channels)\n",
    "        \n",
    "        self.initialize_weights()\n",
    "\n",
    "            \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            # print(m)\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "                '''\n",
    "                # Do not initialize bias (due to batchnorm)-\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                '''\n",
    "            \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                # Standard initialization for batch normalization-\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.dropout(F.relu(self.bn2(self.conv2(y))))\n",
    "        \n",
    "        if self.use_1x1_conv:\n",
    "            x = self.bn3(self.conv3(x))\n",
    "            \n",
    "        y += x\n",
    "        return F.relu(self.dropout(y))\n",
    "    \n",
    "    \n",
    "    def shape_computation(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        print(f\"First conv layer output shape: {y.shape}\")\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        print(f\"Second conv layer output shape: {y.shape}\")\n",
    "        \n",
    "        if self.use_1x1_conv:\n",
    "            x = self.bn3(self.conv3(x))\n",
    "            print(f\"Downsample with S = 2; identity connection output shape: {x.shape}\")\n",
    "            \n",
    "        y += x\n",
    "        print(f\"Residual block output shape: {y.shape}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d3264d-4dd2-434b-a770-e9317da73586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 3, out_channels = 64,\n",
    "            kernel_size = 3, padding = 1,\n",
    "            stride = 1, bias = False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = 64)\n",
    "        \n",
    "        self.resblock1 = ResNet_Block(\n",
    "            num_inp_channels = 64, num_channels = 64,\n",
    "            stride = 1, dropout = 0.2,\n",
    "            use_1x1_conv = False\n",
    "        )\n",
    "        \n",
    "        self.resblock2 = ResNet_Block(\n",
    "            num_inp_channels = 64, num_channels = 64,\n",
    "            stride = 1, dropout = 0.2,\n",
    "            use_1x1_conv = False\n",
    "        )\n",
    "        \n",
    "        # Downsample-\n",
    "        self.resblock3 = ResNet_Block(\n",
    "            num_inp_channels = 64, num_channels = 128,\n",
    "            stride = 2, dropout = 0.2,\n",
    "            use_1x1_conv = True\n",
    "        )\n",
    "        \n",
    "        self.resblock4 = ResNet_Block(\n",
    "            num_inp_channels = 128, num_channels = 128,\n",
    "            stride = 1, dropout = 0.2,\n",
    "            use_1x1_conv = False\n",
    "        )\n",
    "\n",
    "        # Downsample-\n",
    "        self.resblock5 = ResNet_Block(\n",
    "            num_inp_channels = 128, num_channels = 256,\n",
    "            stride = 2, dropout = 0.2,\n",
    "            use_1x1_conv = True\n",
    "        )\n",
    "\n",
    "        self.resblock6 = ResNet_Block(\n",
    "            num_inp_channels = 256, num_channels = 256,\n",
    "            stride = 1, dropout = 0.2,\n",
    "            use_1x1_conv = False\n",
    "        )\n",
    "\n",
    "        # Downsample-\n",
    "        self.resblock7 = ResNet_Block(\n",
    "            num_inp_channels = 256, num_channels = 512,\n",
    "            stride = 2, dropout = 0.2,\n",
    "            use_1x1_conv = True\n",
    "        )\n",
    "\n",
    "        self.resblock8 = ResNet_Block(\n",
    "            num_inp_channels = 512, num_channels = 512,\n",
    "            stride = 1, dropout = 0.2,\n",
    "            use_1x1_conv = False\n",
    "        )\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size = 3, stride = 2)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)\n",
    "        x = self.resblock6(x)\n",
    "        x = self.resblock7(x)\n",
    "        x = self.resblock8(x)\n",
    "        x = self.avg_pool(x).squeeze()\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ef45d08-ae63-48c3-a0b6-0cddade3e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ResNet-18 architecture-\n",
    "model = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9886192-5dab-405a-bd60-5c857e475f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check-\n",
    "x, y = next(iter(train_loader))\n",
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66fdf542-6b9b-4103-b956-215d5d98c3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 3, 32, 32]), torch.Size([256]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f18e888e-ab51-4432-b7b1-14ec832e4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baa38272-e519-4a85-9272-cc81611ef427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da28bb39-8a36-45dc-b584-588cdf690ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a8d013-9ac8-4096-88bf-8586904ffe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of layer-wise parameters and total parameters-\n",
    "tot_params = 0\n",
    "for param in model.parameters():\n",
    "    print(f\"layer.shape = {param.shape} has {param.nelement()} parameters\")\n",
    "    tot_params += param.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8193813b-afa5-41f4-a2c5-c7ee025ef21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of parameters in ResNet-18 CNN = {tot_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abc5e2-3a18-4b92-b121-f68f0000e7de",
   "metadata": {},
   "source": [
    "### Train model with learning rate scheduler\n",
    "\n",
    "Training dataset = 50000, batch size = 256, number of training steps/iterations per epoch = 50000 / 256 = 195.3125 = 195\n",
    "\n",
    "After an initial linear learning rate warmup of 13 epochs or 2539 training steps:\n",
    "\n",
    "- For the next 32 epochs, or, 6250 steps - until 43rd epoch, use lr = 0.1.\n",
    "\n",
    "- For the next 25 epochs, or, 4882 steps - until 68th epoch, use lr = 0.01.\n",
    "\n",
    "- For remaining epochs (13 epochs), use lr = 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "520b76f0-0f7e-4c11-bcdc-376b18bdec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [6250, 11132]\n",
    "values = [0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a627f835-dbdd-44e9-8dbf-5f464b344f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer-\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0, momentum = 0.9, weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c80c5-fda5-4648-8bec-bffe7acf41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check-\n",
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60fd6ce2-fede-4027-ab16-11e9d7e045e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_function(step, boundaries = [6250, 11132], values = [0.1, 0.01, 0.001]):\n",
    "    \n",
    "    for idx, bound in enumerate(boundaries):\n",
    "        if step < bound:\n",
    "            return values[idx]\n",
    "\n",
    "    return values[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14cceaf3-3f0c-457e-8b22-ada8ab9aec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class schedule():\n",
    "\n",
    "    def __init__(self, initial_learning_rate = 0.1, warmup_steps = 1000, decay_func = None):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_func = decay_func\n",
    "        self.warmup_step_size = initial_learning_rate/warmup_steps\n",
    "        self.current_lr = 0\n",
    "\n",
    "    def get_lr(self, step):\n",
    "        if step == 0:\n",
    "            return self.current_lr\n",
    "        elif step <= self.warmup_steps:\n",
    "            self.current_lr+=self.warmup_step_size\n",
    "            return self.current_lr\n",
    "        elif step > self.warmup_steps:\n",
    "            if self.decay_func:\n",
    "                return self.decay_func(step)\n",
    "        else:\n",
    "            return self.current_lr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ad3bc9a-6491-4742-9054-9c241de1043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial linear LR warmup: 195.3125 x 13 = 2549 steps in 13 epochs.\n",
    "custom_lr_scheduler = schedule(\n",
    "    initial_learning_rate = 0.1, warmup_steps = 2539,\n",
    "    decay_func = decay_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cf6339d-8888-43a0-9dbf-7555ccc6b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cfdc866-cbd5-41e3-a809-0787e78d1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_progress(model, train_loader):\n",
    "    '''\n",
    "    Function to perform one epoch of training by using 'train_loader'.\n",
    "    Returns loss and number of correct predictions for this epoch.\n",
    "    '''\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(train_loader, unit = 'batch') as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            tepoch.set_description(f\"Training: \")\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model predictions-\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss-\n",
    "            J = loss(outputs, labels)\n",
    "            \n",
    "            # Empty accumulated gradients-\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform backprop-\n",
    "            J.backward()\n",
    "            \n",
    "            # Update parameters-\n",
    "            optimizer.step()\n",
    "            \n",
    "            global step\n",
    "            optimizer.param_groups[0]['lr'] = custom_lr_scheduler.get_lr(step)\n",
    "\n",
    "            step += 1\n",
    "            \n",
    "            # Compute model's performance statistics-\n",
    "            running_loss += J.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(predicted == labels.data)\n",
    "            \n",
    "            tepoch.set_postfix(\n",
    "                loss = running_loss / len(train_dataset),\n",
    "                accuracy = (running_corrects.double().cpu().numpy() / len(train_dataset)) * 100\n",
    "            )\n",
    "            \n",
    "    \n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = (running_corrects.double() / len(train_dataset)) * 100\n",
    "    \n",
    "\n",
    "    # return running_loss, running_corrects\n",
    "    return train_loss, train_acc.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cbcf1a6-e97b-4e78-88df-b0dbbb15b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_progress(model, test_loader):\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    running_loss_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, unit = 'batch') as tepoch:\n",
    "            for images, labels in tepoch:\n",
    "                tepoch.set_description(f\"Validation: \")\n",
    "                \n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Set model to evaluation mode-\n",
    "                model.eval()\n",
    "            \n",
    "                # Predict using trained model-\n",
    "                outputs = model(images)\n",
    "                _, y_pred = torch.max(outputs, 1)\n",
    "                \n",
    "                # Compute validation loss-\n",
    "                J_val = loss(outputs, labels)\n",
    "                \n",
    "                running_loss_val += J_val.item() * labels.size(0)\n",
    "    \n",
    "                # Total number of labels-\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total number of correct predictions-\n",
    "                correct += (y_pred == labels).sum()\n",
    "                \n",
    "                tepoch.set_postfix(\n",
    "                    val_loss = running_loss_val / len(test_dataset),\n",
    "                    val_acc = 100 * (correct.cpu().numpy() / total)\n",
    "                )\n",
    "            \n",
    "        \n",
    "    # return (running_loss_val, correct, total)\n",
    "    val_loss = running_loss_val / len(test_dataset)\n",
    "    val_acc = (correct / total) * 100\n",
    "\n",
    "    return val_loss, val_acc.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71e9b214-dc52-4f32-9f19-4c893735c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 dict to contain training metrics-\n",
    "training_history_lr_scheduler = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5841cc77-bef8-4057-8bb5-1c92ff3f5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters saving 'best' models-\n",
    "best_val_loss = 100\n",
    "loc_patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a1cf9-6e7d-43c7-b2ca-4a6078a04a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Train and validate model for 1 epoch-\n",
    "    train_loss, train_acc = train_model_progress(model, train_loader)\n",
    "    val_loss, val_acc = test_model_progress(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nepoch: {epoch + 1} training loss = {train_loss:.4f}, \"\n",
    "          f\"training accuracy = {train_acc:.2f}%, val_loss = {val_loss:.4f}\"\n",
    "          f\", val_accuracy = {val_acc:.2f}% & \"\n",
    "          f\"LR = {optimizer.param_groups[0]['lr']:.4f}\\n\")\n",
    "    \n",
    "    training_history_lr_scheduler[epoch + 1] = {\n",
    "        'loss': train_loss, 'acc': train_acc,\n",
    "        'val_loss': val_loss, 'val_acc': val_acc,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Save best weights achieved until now-\n",
    "    if (val_loss < best_val_loss):    \n",
    "        # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        print(f\"Saving model with lowest val_loss = {val_loss:.4f}\\n\")\n",
    "        \n",
    "        # Save trained model with 'best' validation accuracy-\n",
    "        torch.save(model.state_dict(), \"ResNet18_best_model.pth\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a274d8f3-2506-4887-896c-af2959a62c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training ResNet-18 CNN on CIFAR-10 dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"Finished training ResNet-18 CNN on CIFAR-10 dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e252b63-3b02-400d-b922-a90ddaac3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model from last training epoch-\n",
    "torch.save(model.state_dict(), \"ResNet18_last_epoch_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b9e9b1c-77bf-4377-b8a3-739b10c14119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training metrics as Python3 history for later analysis-\n",
    "with open(\"ResNet18_training_history_lr_scheduler.pkl\", \"wb\") as file:\n",
    "    pickle.dump(training_history_lr_scheduler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8738816d-3ef3-4bbe-a0d9-8b5e06938b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27bf12d7-3f27-4542-ba6c-e51eb2ede4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and load 'best' trained parameters from above-\n",
    "best_trained_model = ResNet18().to(device)\n",
    "best_trained_model.load_state_dict(torch.load('ResNet18_best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbb4256f-61d1-47d4-af12-f4cc370a06c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: : 100%|████████████████████████████████████████████████████████████████████| 40/40 [00:01<00:00, 23.02batch/s, val_acc=93.1, val_loss=0.312]\n"
     ]
    }
   ],
   "source": [
    "# Get validation loss and accuracy-\n",
    "val_loss, val_acc = test_model_progress(best_trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5b9a230-83a0-4990-8f67-0b99a28e4a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-18 'best' model metrics: val_loss = 0.3118 & val_acc = 93.14%\n"
     ]
    }
   ],
   "source": [
    "print(f\"ResNet-18 'best' model metrics: val_loss = {val_loss:.4f} & val_acc = {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8119a41d-ef4b-4cf9-b697-7bd2f4204ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "del best_trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d3dd7-3d0b-4eba-b65d-501526fffb13",
   "metadata": {},
   "source": [
    "### ResNet-18 training visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
